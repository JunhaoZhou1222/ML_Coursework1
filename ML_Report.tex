\documentclass{article}
\usepackage{graphicx}
\usepackage{float}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{hyperref}
\geometry{left=2.5cm, right=2.5cm,top=1.8cm,bottom=1.8cm}

\title{ML Coursework1 Report}
\author{Junhao Zhou (k23172173) }
\date{February 16th 2026}

\begin{document}

\maketitle
\noindent \textbf{GitHub Repository:} \\
\href{https://github.com/JunhaoZhou1222/ML_Coursework1}{https://github.com/JunhaoZhou1222/ML\_Coursework1}

\section{Introduction}
The objective of this course assignment is to establish a reliable model to predict the target variable "Result" based on the diamond feature dataset. This dataset includes physical attributes (carat weight, depth), categorical features (cut, color, clarity), and several features.

\section{Exploratory data analysis}
\begin{enumerate}
    \item \textbf{Logical Constraints (Carat):} Instances with $carat \le 0$ or $carat > 5$ were removed as these represent data entry errors or extreme outliers irrelevant to the general distribution.
    \item \textbf{Physical Constraints (Dimensions):} Rows where $x$, $y$, or $z$ were zero were removed as diamonds cannot have zero volume.
    \item \textbf{Statistical Outliers (IQR Method):} For features `depth` and `table`, the Interquartile Range (IQR) method was applied. Data points falling outside $Q1 - 1.5 \times IQR$ and $Q3 + 1.5 \times IQR$ were identified as noise.
\end{enumerate}

Figure \ref{fig:cleaning} demonstrates the effectiveness of this process. The boxplots show the regularization of feature distributions, while the scatter plot reveals the removal of sparse, unrealistic data points, resulting in a dense, cohesive cluster for training.

\begin{figure}[H]
    \centering
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{viz_comparison_boxplots.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{viz_comparison_scatter.png}
    \end{minipage}
    \caption{Data cleaning visualization. Left: Comparison of feature distributions before and after IQR cleaning, showing regularization of Depth and Table. Right: Bivariate analysis showing removal of low-density artifacts (red) to preserve high-density core data (green).}
    \label{fig:cleaning}
\end{figure}

\section{Model selection}
After evaluating multiple candidate algorithms, the \textbf{XGBoost (Extreme Gradient Boosting) regressor} was selected as the final model. This choice was based on its superior predictive performance on the validation set compared to other ensemble methods and baseline models.

\subsection{Dataset Characteristics}
The initial exploratory data analysis revealed several important patterns:
\begin{itemize}
    \item \textbf{Mixed feature types}: The dataset contains both continuous numerical variables and categorical variables (e.g., cut, color, clarity), requiring a model capable of handling heterogeneous data effectively.
    \item \textbf{Non-linear relationships}: Preliminary analysis suggested complex, non-linear interactions between dimensions ($x, y, z$) and price, which linear models struggle to capture.
\end{itemize}

\subsection{Comparison of Different Models}
Several regression algorithms were considered:

\vspace{0.3cm}

\noindent\textbf{Linear Regression (Baseline):} While computationally efficient and interpretable, linear regression assumes a linear relationship between features and the target. Given the observed non-linearities in the data, this model served primarily as a baseline and achieved the lowest performance ($R^2 \approx 0.32$).

\vspace{0.3cm}

\noindent\textbf{Random Forest:} An ensemble of bagging decision trees. While it handles non-linearity well and is robust to outliers, it produced a lower validation score ($R^2 \approx 0.45$) compared to the boosting approach. It also lacks the fine-grained regularization capabilities found in XGBoost.

\vspace{0.3cm}

\noindent\textbf{XGBoost (Selected):} XGBoost is a gradient boosting framework that builds trees sequentially, with each new tree correcting errors made by previous ones. It was selected for the following reasons:
\begin{itemize}
    \item \textbf{Superior Performance}: It achieved the highest $R^2$ score ($0.47$) among all tested models.
    \item \textbf{Regularization}: Unlike standard gradient boosting, XGBoost includes L1 and L2 regularization, which helps prevent overfitting on the training data.
    \item \textbf{Gradient Boosting Framework}: By optimizing a differentiable loss function, it captures subtle patterns in the diamond pricing structure that bagging methods (like Random Forest) missed.
\end{itemize}

\section{Model training and evaluation}
The final XGBoost model was trained using the following hyperparameters: $n\_estimators=200$, $learning\_rate=0.1$, and $random\_state=123$. To evaluate the model's generalization capability, the dataset was split into training and validation sets (80\% training, 20\% validation).

\begin{itemize}
    \item \textbf{$n\_estimators = 200$:} The number of boosting rounds (trees). A value of 200 was chosen to allow the model sufficient iterations to correct residual errors without leading to significant overfitting.
    
    \item \textbf{$learning\_rate = 0.1$:} This controls the step size shrinkage used to prevent overfitting. A moderate rate of 0.1 ensures the model converges stably to the optimal solution.
\end{itemize}

\begin{table}[H]
    \centering
    \caption{Model Performance Summary}
    \begin{tabular}{lc}
        \toprule
        \textbf{Metric} & \textbf{Score ($R^2$)} \\
        \midrule
        Training $R^2$ & 0.92486 \\
        \textbf{Random forest Score (Estimated $R^2$)} & \textbf{0.46154} \\
        \textbf{Linear Regression Score (Estimated $R^2$)} & \textbf{0.32128} \\
        \textbf{XGboost Score (Estimated $R^2$)} & \textbf{0.47015} \\
        \bottomrule
    \end{tabular}
    \label{tab:results}
\end{table}

As shown in Table \ref{tab:results}, The training score (0.92) indicates the model has learned the training data well, the XGboost score (0.47) is the best performance model on the validation set.
\end{document}
